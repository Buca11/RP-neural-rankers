{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.faiss import FaissSearcher, TctColBertQueryEncoder\n",
    "import ir_datasets\n",
    "import pandas as pd\n",
    "from fast_forward import Ranking\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('queries.csv',names=['none','q_id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TctColBertQueryEncoder('castorini/tct_colbert-msmarco')\n",
    "dsearcher = FaissSearcher('/home/lucia/research_project/in4325-information-retrieval/intro-pyterrier/query-latency/dense_index_fiqa',encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate results and add to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9 s ± 198 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "query_ids = []\n",
    "doc_ids = []\n",
    "scores = []\n",
    "rows = sample.iterrows()\n",
    "next(rows)\n",
    "for query in rows:\n",
    "    hits = dsearcher.search(query[1].text,k=100)\n",
    "    for hit in hits:\n",
    "        query_ids.append(query[1].q_id)\n",
    "        doc_ids.append(hit.docid)\n",
    "        scores.append(hit.score)\n",
    "df = pd.DataFrame({\n",
    "    'q_id': query_ids,\n",
    "    'id': doc_ids,\n",
    "    'score': scores\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_missing_scores_and_interpolate(dense_ranking, sparse_ranking, method, alpha, normalize):\n",
    "    full = None\n",
    "    if (method == 'zero'):\n",
    "        merged = dense_ranking.merge(sparse_ranking, on=[\"q_id\", \"id\"],how = 'outer', suffixes=['_dense', \"_sparse\"])\n",
    "        merged['score_dense'] = merged['score_dense'].fillna(0)\n",
    "        merged['score_sparse'] = merged['score_sparse'].fillna(0)\n",
    "        full = merged\n",
    "    if (method == 'average'):\n",
    "        merged = dense_ranking.merge(sparse_ranking, on=[\"q_id\", \"id\"],how = 'outer', suffixes=['_dense', \"_sparse\"])\n",
    "        merged['score_dense'] = merged['score_dense'].fillna(merged[\"score_dense\"].mean())\n",
    "        merged['score_sparse'] = merged['score_sparse'].fillna(merged[\"score_sparse\"].mean())\n",
    "        full = merged\n",
    "    if (method == 'drop'):\n",
    "        merged = dense_ranking.merge(sparse_ranking, on=[\"q_id\", \"id\"],how = 'inner', suffixes=['_dense', \"_sparse\"])\n",
    "        full = merged\n",
    "    if (method == 'median'):\n",
    "        merged = dense_ranking.merge(sparse_ranking, on=[\"q_id\", \"id\"],how = 'outer', suffixes=['_dense', \"_sparse\"])\n",
    "        merged['score_dense'] = merged['score_dense'].fillna(merged[\"score_dense\"].median())\n",
    "        merged['score_sparse'] = merged['score_sparse'].fillna(merged[\"score_sparse\"].median())\n",
    "        full = merged\n",
    "    if normalize:\n",
    "        normalize_column(full,'score_dense')\n",
    "        normalize_column(full,'score_sparse')\n",
    "\n",
    "    full[\"score\"] = alpha * full['score_sparse'] + (1 - alpha) * full['score_dense']\n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(searcher,sample):\n",
    "    query_ids = []\n",
    "    doc_ids = []\n",
    "    scores = []\n",
    "    rows = sample.iterrows()\n",
    "    next(rows)\n",
    "    for query in rows:\n",
    "        hits = searcher.search(query[1].text,k=100)\n",
    "        for hit in hits:\n",
    "            query_ids.append(query[1].q_id)\n",
    "            doc_ids.append(hit.docid)\n",
    "            scores.append(hit.score)\n",
    "    df = pd.DataFrame({\n",
    "        'q_id': query_ids,\n",
    "        'id': doc_ids,\n",
    "        'score': scores\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = generate_df(dsearcher,sample)\n",
    "dfs = Ranking.from_file(Path('/home/lucia/research_project/in4325-information-retrieval/intro-pyterrier/query-latency/trec-run-bm25-fiqa-latency.txt'))._df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.2 ms ± 660 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'zero',0.1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.91 ms ± 215 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'drop',0.1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5 ms ± 348 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'median',0.1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.1 ms ± 225 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'average',0.1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, column_name):\n",
    "    min_val = df[column_name].min()\n",
    "    max_val = df[column_name].max()\n",
    "    df[column_name] = (df[column_name] - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.39 ms ± 237 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'drop',0.1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 ms ± 725 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'zero',0.1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 ms ± 278 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'median',0.1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5 ms ± 252 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r 7 -n 100\n",
    "deal_with_missing_scores_and_interpolate(dfd,dfs,'average',0.1,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
